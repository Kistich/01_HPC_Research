#!/usr/bin/env python3
"""
ä¸€æœŸäºŒæœŸæ•°æ®è¿‡æ»¤å™¨
åŸºäºè„šæœ¬ç‰¹å¾å’ŒèŠ‚ç‚¹ä¿¡æ¯è¿›è¡Œç²¾ç¡®åˆ†ç±»
"""

import pandas as pd
import numpy as np
import re
import yaml
import logging
from typing import Dict, List, Tuple, Optional, Any
import os
import sys
from pathlib import Path

# æ·»åŠ utilsè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "utils"))
from parallel_processor import ParallelProcessor
from progress_tracker import ProgressTracker

logger = logging.getLogger(__name__)

class GenerationFilter:
    """ä¸€æœŸäºŒæœŸæ•°æ®è¿‡æ»¤å™¨"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è¿‡æ»¤å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.processor = ParallelProcessor(
            max_cores=self.config['parallel_processing']['max_cores'],
            memory_limit_gb=self.config['parallel_processing']['memory_limit_gb']
        )
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compile_patterns()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_jobs': 0,
            'second_generation_high': 0,
            'second_generation_medium': 0,
            'second_generation_low': 0,
            'first_generation': 0,
            'mixed_features': 0,
            'management_nodes': 0,
            'unknown_category': 0
        }
        
        logger.info("ä¸€æœŸäºŒæœŸæ•°æ®è¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            return config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _compile_patterns(self):
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        self.first_gen_patterns = []
        self.second_gen_patterns = []
        self.mgmt_node_patterns = []
        self.first_gen_node_patterns = []
        self.second_gen_node_patterns = []
        
        # ç¼–è¯‘è„šæœ¬ç‰¹å¾æ¨¡å¼
        for pattern in self.config['generation_filter']['script_features']['first_generation_patterns']:
            self.first_gen_patterns.append(re.compile(pattern, re.IGNORECASE))
        
        for pattern in self.config['generation_filter']['script_features']['second_generation_patterns']:
            self.second_gen_patterns.append(re.compile(pattern, re.IGNORECASE))
        
        # ç¼–è¯‘èŠ‚ç‚¹æ¨¡å¼
        for pattern in self.config['generation_filter']['node_classification']['management_nodes']:
            self.mgmt_node_patterns.append(re.compile(pattern, re.IGNORECASE))

        # æ˜ç¡®çš„äºŒæœŸèŠ‚ç‚¹æ¨¡å¼ - æœ€é«˜ä¼˜å…ˆçº§
        self.definitive_second_gen_patterns = []
        for pattern in self.config['generation_filter']['node_classification']['definitive_second_generation_nodes']:
            self.definitive_second_gen_patterns.append(re.compile(pattern, re.IGNORECASE))

        # å¯èƒ½çš„ä¸€æœŸèŠ‚ç‚¹æ¨¡å¼
        self.possible_first_gen_patterns = []
        for pattern in self.config['generation_filter']['node_classification']['possible_first_generation_nodes']:
            self.possible_first_gen_patterns.append(re.compile(pattern, re.IGNORECASE))

        # å¯èƒ½çš„äºŒæœŸèŠ‚ç‚¹æ¨¡å¼
        self.possible_second_gen_patterns = []
        for pattern in self.config['generation_filter']['node_classification']['possible_second_generation_nodes']:
            self.possible_second_gen_patterns.append(re.compile(pattern, re.IGNORECASE))
        
        logger.info("æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç¼–è¯‘å®Œæˆ")
    
    def filter_data(self, input_file: str, output_dir: str) -> Dict[str, str]:
        """
        æ‰§è¡Œæ•°æ®è¿‡æ»¤
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        logger.info(f"å¼€å§‹ä¸€æœŸäºŒæœŸæ•°æ®è¿‡æ»¤: {input_file}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # å®šä¹‰ä¸­é—´æ–‡ä»¶è·¯å¾„
        analyzed_file = os.path.join(output_dir, "analyzed_features.csv")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åˆ†æç»“æœæ–‡ä»¶
        if os.path.exists(analyzed_file):
            logger.info(f"âœ… å‘ç°ä¸­é—´æ–‡ä»¶: {analyzed_file}")
            file_size_mb = os.path.getsize(analyzed_file) / (1024 * 1024)
            logger.info(f"æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")

            try:
                # åŠ è½½å·²åˆ†æçš„æ•°æ®
                logger.info("åŠ è½½å·²åˆ†æçš„ç‰¹å¾æ•°æ®...")
                analyzed_df = pd.read_csv(analyzed_file)
                # è®¾ç½®æ€»ä½œä¸šæ•°ç»Ÿè®¡
                self.stats['total_jobs'] = len(analyzed_df)
                logger.info(f"âœ… æˆåŠŸåŠ è½½åˆ†æç»“æœ: {len(analyzed_df):,} æ¡è®°å½•")
                logger.info("â­ï¸  è·³è¿‡è„šæœ¬ç‰¹å¾åˆ†æï¼Œç›´æ¥è¿›è¡Œåˆ†ç±»å†³ç­–...")

            except Exception as e:
                logger.warning(f"âš ï¸  åŠ è½½ä¸­é—´æ–‡ä»¶å¤±è´¥: {e}")
                logger.info("ğŸ”„ é‡æ–°è¿›è¡Œå®Œæ•´åˆ†æ...")
                analyzed_df = self._perform_full_analysis(input_file, analyzed_file)
        else:
            logger.info("âŒ æœªå‘ç°ä¸­é—´æ–‡ä»¶ï¼Œè¿›è¡Œå®Œæ•´åˆ†æ...")
            analyzed_df = self._perform_full_analysis(input_file, analyzed_file)
        
        # æ‰§è¡Œåˆ†ç±»å†³ç­–
        logger.info("æ‰§è¡Œåˆ†ç±»å†³ç­–...")
        classified_df = self._make_classification_decisions(analyzed_df)
        
        # åˆ†ç¦»ä¸åŒç±»åˆ«çš„æ•°æ®
        output_files = self._separate_categories(classified_df, output_dir)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_classification_report(output_dir)
        
        logger.info("ä¸€æœŸäºŒæœŸæ•°æ®è¿‡æ»¤å®Œæˆ")
        return output_files

    def _perform_full_analysis(self, input_file: str, analyzed_file: str) -> pd.DataFrame:
        """æ‰§è¡Œå®Œæ•´çš„è„šæœ¬ç‰¹å¾åˆ†æå¹¶ä¿å­˜ä¸­é—´ç»“æœ"""
        # åŠ è½½æ•°æ®
        logger.info("åŠ è½½æ•°æ®...")
        df = pd.read_csv(input_file)
        self.stats['total_jobs'] = len(df)

        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} æ¡è®°å½•")

        # åˆ†å—å¤„ç†
        chunks = self.processor.split_dataframe(df)

        # å¹¶è¡Œåˆ†æè„šæœ¬ç‰¹å¾å’ŒèŠ‚ç‚¹ä¿¡æ¯
        with ProgressTracker(len(chunks), "è„šæœ¬ç‰¹å¾åˆ†æ", "å—") as pbar:
            def progress_callback(completed, total):
                pbar.update(1)

            results = []
            for i, chunk in enumerate(chunks):
                result = self._analyze_chunk(chunk)
                results.append(result)
                progress_callback(i + 1, len(chunks))

        # åˆå¹¶ç»“æœ
        logger.info("åˆå¹¶åˆ†æç»“æœ...")
        analyzed_df = self.processor.merge_results(results)

        # ä¿å­˜ä¸­é—´ç»“æœ
        logger.info(f"ğŸ’¾ ä¿å­˜åˆ†æç»“æœåˆ°: {analyzed_file}")
        analyzed_df.to_csv(analyzed_file, index=False)
        file_size_mb = os.path.getsize(analyzed_file) / (1024 * 1024)
        logger.info(f"âœ… ä¸­é—´æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_size_mb:.1f} MB")

        return analyzed_df

    def _analyze_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ†ææ•°æ®å—çš„è„šæœ¬ç‰¹å¾å’ŒèŠ‚ç‚¹ä¿¡æ¯

        Args:
            chunk: æ•°æ®å—

        Returns:
            åˆ†æåçš„æ•°æ®å—
        """
        chunk = chunk.copy()
        
        # åˆå§‹åŒ–åˆ†æç»“æœåˆ—
        chunk['first_gen_script_score'] = 0.0
        chunk['second_gen_script_score'] = 0.0
        chunk['definitive_second_gen_score'] = 0.0  # æ˜ç¡®äºŒæœŸèŠ‚ç‚¹åˆ†æ•°
        chunk['possible_first_gen_score'] = 0.0     # å¯èƒ½ä¸€æœŸèŠ‚ç‚¹åˆ†æ•°
        chunk['possible_second_gen_score'] = 0.0    # å¯èƒ½äºŒæœŸèŠ‚ç‚¹åˆ†æ•°
        chunk['is_management_node'] = False
        chunk['node_classification_decisive'] = False  # èŠ‚ç‚¹åˆ†ç±»æ˜¯å¦å…·æœ‰å†³å®šæ€§
        
        # åˆ†æè„šæœ¬ç‰¹å¾ - å‘é‡åŒ–å¤„ç†
        # æ„å»ºè„šæœ¬å†…å®¹ - å®‰å…¨çš„å­—ç¬¦ä¸²æ‹¼æ¥
        command_series = chunk.get('command', pd.Series([''] * len(chunk))).fillna('').astype(str)
        job_name_series = chunk.get('job_name', pd.Series([''] * len(chunk))).fillna('').astype(str)
        script_contents = command_series + ' ' + job_name_series

        # å‘é‡åŒ–è®¡ç®—è„šæœ¬åˆ†æ•°
        chunk['first_gen_script_score'] = script_contents.apply(
            lambda x: self._calculate_script_score(x, self.first_gen_patterns)
        )
        chunk['second_gen_script_score'] = script_contents.apply(
            lambda x: self._calculate_script_score(x, self.second_gen_patterns)
        )

        # åˆ†æèŠ‚ç‚¹ä¿¡æ¯ - å‘é‡åŒ–å¤„ç†
        exec_hosts_series = chunk.get('exec_hosts', '').fillna('').astype(str)
        first_exec_host_series = chunk.get('first_exec_host', '').fillna('').astype(str)
        from_host_series = chunk.get('from_host', '').fillna('').astype(str)

        # å‘é‡åŒ–æ£€æŸ¥ç®¡ç†èŠ‚ç‚¹ - å®‰å…¨çš„å­—ç¬¦ä¸²è½¬æ¢
        def safe_str(value):
            """å®‰å…¨çš„å­—ç¬¦ä¸²è½¬æ¢ï¼Œå¤„ç†NaNå’Œfloat"""
            if pd.isna(value):
                return ''
            return str(value)

        chunk['is_management_node'] = chunk.apply(
            lambda row: self._check_management_node(
                safe_str(row.get('exec_hosts', '')),
                safe_str(row.get('first_exec_host', '')),
                safe_str(row.get('from_host', ''))
            ), axis=1
        )

        # å¯¹éç®¡ç†èŠ‚ç‚¹è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ
        non_mgmt_mask = ~chunk['is_management_node']
        non_mgmt_chunk = chunk[non_mgmt_mask].copy()

        if len(non_mgmt_chunk) > 0:
            # å‘é‡åŒ–è®¡ç®—æ˜ç¡®çš„äºŒæœŸèŠ‚ç‚¹åˆ†æ•°
            chunk.loc[non_mgmt_mask, 'definitive_second_gen_score'] = non_mgmt_chunk.apply(
                lambda row: self._calculate_node_score(
                    safe_str(row.get('exec_hosts', '')),
                    safe_str(row.get('first_exec_host', '')),
                    safe_str(row.get('from_host', '')),
                    self.definitive_second_gen_patterns
                ), axis=1
            )

            # æ ‡è®°å†³å®šæ€§åˆ†ç±»
            decisive_mask = chunk['definitive_second_gen_score'] > 0
            chunk.loc[decisive_mask, 'node_classification_decisive'] = True

            # å¯¹éå†³å®šæ€§çš„è®°å½•è®¡ç®—å¯èƒ½çš„èŠ‚ç‚¹åˆ†æ•°
            non_decisive_mask = non_mgmt_mask & ~decisive_mask
            non_decisive_chunk = chunk[non_decisive_mask].copy()

            if len(non_decisive_chunk) > 0:
                # è®¡ç®—å¯èƒ½çš„ä¸€æœŸèŠ‚ç‚¹åˆ†æ•°
                chunk.loc[non_decisive_mask, 'possible_first_gen_score'] = non_decisive_chunk.apply(
                    lambda row: self._calculate_node_score(
                        safe_str(row.get('exec_hosts', '')),
                        safe_str(row.get('first_exec_host', '')),
                        safe_str(row.get('from_host', '')),
                        self.possible_first_gen_patterns
                    ), axis=1
                )

                # è®¡ç®—å¯èƒ½çš„äºŒæœŸèŠ‚ç‚¹åˆ†æ•°
                chunk.loc[non_decisive_mask, 'possible_second_gen_score'] = non_decisive_chunk.apply(
                    lambda row: self._calculate_node_score(
                        safe_str(row.get('exec_hosts', '')),
                        safe_str(row.get('first_exec_host', '')),
                        safe_str(row.get('from_host', '')),
                        self.possible_second_gen_patterns
                    ), axis=1
                )
        
        return chunk
    
    def _calculate_script_score(self, script_content: str, patterns: List[re.Pattern]) -> float:
        """è®¡ç®—è„šæœ¬ç‰¹å¾åˆ†æ•°"""
        if not script_content or script_content == 'nan':
            return 0.0
        
        score = 0.0
        weights = self.config['generation_filter']['script_features']['feature_weights']
        
        for pattern in patterns:
            matches = pattern.findall(script_content)
            if matches:
                # æ ¹æ®æ¨¡å¼ç±»å‹åˆ†é…æƒé‡
                if pattern.pattern.startswith('#'):
                    score += weights['script_header']
                elif any(cmd in pattern.pattern for cmd in ['jsub', 'bsub', 'sbatch', 'srun']):
                    score += weights['command_type']
                elif any(res in pattern.pattern for res in ['-n', '-R', '--ntasks', '--mem']):
                    score += weights['resource_syntax']
                elif any(env in pattern.pattern for env in ['LSF_', 'SLURM_']):
                    score += weights['environment_var']
                else:
                    score += weights['queue_syntax']
        
        return score
    
    def _check_management_node(self, exec_hosts: str, first_exec_host: str, from_host: str = '') -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†èŠ‚ç‚¹"""
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½æ˜¯å­—ç¬¦ä¸²
        exec_hosts = str(exec_hosts) if exec_hosts is not None else ''
        first_exec_host = str(first_exec_host) if first_exec_host is not None else ''
        from_host = str(from_host) if from_host is not None else ''

        for pattern in self.mgmt_node_patterns:
            if (pattern.search(exec_hosts) or
                pattern.search(first_exec_host) or
                pattern.search(from_host)):
                return True
        return False
    
    def _calculate_node_score(self, exec_hosts: str, first_exec_host: str, from_host: str, patterns: List[re.Pattern]) -> float:
        """è®¡ç®—èŠ‚ç‚¹åˆ†æ•°"""
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½æ˜¯å­—ç¬¦ä¸²
        exec_hosts = str(exec_hosts) if exec_hosts is not None else ''
        first_exec_host = str(first_exec_host) if first_exec_host is not None else ''
        from_host = str(from_host) if from_host is not None else ''

        score = 0.0
        weights = self.config['generation_filter']['node_classification']['node_weights']

        for pattern in patterns:
            if pattern.search(exec_hosts):
                score += weights.get('exec_hosts', 2.0)
            if pattern.search(first_exec_host):
                score += weights.get('first_exec_host', 1.5)
            if pattern.search(from_host):
                score += weights.get('from_host', 1.0)  # from_hostæƒé‡è¾ƒä½

        return score

    def _apply_cluster_filtering(self, df: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨é›†ç¾¤è¿‡æ»¤ï¼Œç§»é™¤ä¸€æœŸé›†ç¾¤æ•°æ®"""
        if 'cluster_filtering' not in self.config['generation_filter']:
            return df

        cluster_config = self.config['generation_filter']['cluster_filtering']
        excluded_clusters = cluster_config.get('excluded_clusters', [])

        if not excluded_clusters or 'cluster_name' not in df.columns:
            return df

        original_count = len(df)

        # è¿‡æ»¤æ‰æ’é™¤çš„é›†ç¾¤
        mask = ~df['cluster_name'].isin(excluded_clusters)
        filtered_df = df[mask].copy()

        filtered_count = len(filtered_df)
        excluded_count = original_count - filtered_count

        if excluded_count > 0:
            logger.info(f"é›†ç¾¤è¿‡æ»¤: ç§»é™¤ {excluded_count:,} æ¡ä¸€æœŸé›†ç¾¤è®°å½• ({excluded_clusters})")

        return filtered_df

    def _parse_exec_hosts_detailed(self, exec_hosts_str: str) -> Dict[str, Any]:
        """è¯¦ç»†è§£æexec_hostså­—æ®µï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if pd.isna(exec_hosts_str) or exec_hosts_str == '':
            return {
                'hosts': [],
                'node_count': 0,
                'primary_subcluster': 'unknown',
                'is_gpu_job': False,
                'subcluster_distribution': {}
            }

        exec_hosts_str = str(exec_hosts_str).strip()
        hosts = []

        # 1. å¤„ç†ç©ºæ ¼åˆ†éš”æ ¼å¼: "cpu1-01 cpu1-02 cpu1-03"
        if ' ' in exec_hosts_str and '+' not in exec_hosts_str and '[' not in exec_hosts_str:
            hosts = [h.strip() for h in exec_hosts_str.split() if h.strip()]

        # 2. å¤„ç†èŒƒå›´æ ¼å¼: cpu1-[01-05] æˆ– gpu1-[1-8]
        elif '[' in exec_hosts_str and ']' in exec_hosts_str:
            import re
            match = re.match(r'([a-zA-Z0-9_-]+)\[(\d+)-(\d+)\]', exec_hosts_str)
            if match:
                prefix, start, end = match.groups()
                start_num, end_num = int(start), int(end)
                # ä¿æŒåŸå§‹æ ¼å¼çš„é›¶å¡«å……
                if len(start) == len(end) and len(start) > 1:
                    hosts = [f"{prefix}{i:0{len(start)}d}" for i in range(start_num, end_num + 1)]
                else:
                    hosts = [f"{prefix}{i}" for i in range(start_num, end_num + 1)]

        # 3. å¤„ç†åŠ å·åˆ†éš”æ ¼å¼: "cpu1-01+cpu1-02+cpu1-03"
        elif '+' in exec_hosts_str:
            hosts = [h.strip() for h in exec_hosts_str.split('+') if h.strip()]

        # 4. å¤„ç†é‡å¤æ ¼å¼: "gpu1-31 gpu1-31 gpu1-31 gpu1-31"
        elif ' ' in exec_hosts_str:
            hosts = [h.strip() for h in exec_hosts_str.split() if h.strip()]

        # 5. å•ä¸ªä¸»æœº
        else:
            hosts = [exec_hosts_str]

        # åˆ†æä¸»æœºä¿¡æ¯
        node_count = len(hosts)
        subcluster_distribution = {}
        is_gpu_job = False

        for host in hosts:
            # è¯†åˆ«å­é›†ç¾¤
            subcluster = self._identify_subcluster(host)
            subcluster_distribution[subcluster] = subcluster_distribution.get(subcluster, 0) + 1

            # æ£€æŸ¥æ˜¯å¦ä¸ºGPUä½œä¸šï¼ˆæ£€æŸ¥ä¸»æœºåä¸­æ˜¯å¦åŒ…å«gpuï¼‰
            if 'gpu' in host.lower():
                is_gpu_job = True

        # ç¡®å®šä¸»è¦å­é›†ç¾¤
        primary_subcluster = max(subcluster_distribution.keys(),
                               key=lambda k: subcluster_distribution[k]) if subcluster_distribution else 'unknown'

        return {
            'hosts': hosts,
            'node_count': node_count,
            'primary_subcluster': primary_subcluster,
            'is_gpu_job': is_gpu_job,
            'subcluster_distribution': subcluster_distribution
        }

    def _identify_subcluster(self, hostname: str) -> str:
        """è¯†åˆ«ä¸»æœºæ‰€å±çš„å­é›†ç¾¤"""
        hostname = hostname.lower()

        # GPUé›†ç¾¤
        if hostname.startswith('gpu1-'):
            return 'GPU1'
        elif hostname.startswith('gpu2-'):
            return 'GPU2'
        elif hostname.startswith('gpu3-'):
            return 'GPU3'

        # CPUé›†ç¾¤
        elif hostname.startswith('cpu1-'):
            return 'CPU1'
        elif hostname.startswith('cpu2-'):
            return 'CPU2'
        elif hostname.startswith('cpu3-'):
            return 'CPU3'

        # å¤§å†…å­˜é›†ç¾¤
        elif hostname.startswith('bigmem-') or hostname.startswith('bigmen-'):
            return 'BIGMEM'

        # å…¶ä»–
        else:
            return 'unknown'

    def _calculate_accurate_resources(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å‡†ç¡®çš„CPUå’ŒGPUèµ„æº"""
        logger.info("è®¡ç®—å‡†ç¡®çš„CPUå’ŒGPUèµ„æº...")

        df = df.copy()
        resource_config = self.config.get('resource_calculation', {})
        cpu_configs = resource_config.get('cpu_configs', {})
        gpu_configs = resource_config.get('gpu_configs', {})

        # åˆå§‹åŒ–èµ„æºå­—æ®µ
        df['accurate_cpu_cores'] = 0
        df['accurate_gpu_count'] = 0
        df['accurate_node_count'] = 0
        df['gpu_type'] = ''
        df['subcluster_type'] = ''

        for idx, row in df.iterrows():
            exec_info = self._parse_exec_hosts_detailed(row.get('exec_hosts', ''))
            primary_subcluster = exec_info['primary_subcluster']
            node_count = exec_info['node_count']

            df.at[idx, 'accurate_node_count'] = node_count
            df.at[idx, 'subcluster_type'] = primary_subcluster

            # è®¡ç®—CPUèµ„æº
            if primary_subcluster in cpu_configs:
                cores_per_node = cpu_configs[primary_subcluster]['cores_per_node']
                df.at[idx, 'accurate_cpu_cores'] = cores_per_node * node_count
            elif primary_subcluster in gpu_configs:
                cores_per_node = gpu_configs[primary_subcluster]['cpu_cores_per_node']
                df.at[idx, 'accurate_cpu_cores'] = cores_per_node * node_count
            else:
                # ä½¿ç”¨num_processorsä½œä¸ºå¤‡é€‰
                df.at[idx, 'accurate_cpu_cores'] = row.get('num_processors', 0)

            # è®¡ç®—GPUèµ„æº
            if primary_subcluster in gpu_configs and exec_info['is_gpu_job']:
                gpu_config = gpu_configs[primary_subcluster]
                gpus_per_node = gpu_config['gpus_per_node']
                df.at[idx, 'accurate_gpu_count'] = gpus_per_node * node_count
                df.at[idx, 'gpu_type'] = gpu_config['gpu_type']

        logger.info("èµ„æºè®¡ç®—å®Œæˆ")
        return df

    def _make_classification_decisions(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‰§è¡Œåˆ†ç±»å†³ç­– - ä½¿ç”¨å‘é‡åŒ–æ“ä½œå’Œå¹¶è¡Œå¤„ç†"""
        logger.info(f"å¼€å§‹åˆ†ç±»å†³ç­–å¤„ç†: {len(df):,} æ¡è®°å½•")

        # ä½¿ç”¨å¹¶è¡Œå¤„ç†è¿›è¡Œåˆ†ç±»å†³ç­–
        chunk_size = 50000  # æ¯ä¸ªå—å¤„ç†5ä¸‡æ¡è®°å½•
        chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]

        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ: {len(chunks)}ä¸ªå—, æ¯å—çº¦{chunk_size:,}è¡Œ")

        # å¹¶è¡Œå¤„ç†å„ä¸ªå—
        with ProgressTracker(len(chunks), "åˆ†ç±»å†³ç­–å¤„ç†") as tracker:
            # åˆ›å»ºè¿›åº¦å›è°ƒé€‚é…å™¨
            last_current = [0]  # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨å¯å˜å€¼
            def progress_adapter(current, total):
                # è®¡ç®—å¢é‡å¹¶æ›´æ–°
                increment = current - last_current[0]
                if increment > 0:
                    tracker.update(increment)
                    last_current[0] = current

            results = self.processor.process_chunks_with_pool(
                chunks,
                self._classify_chunk,
                progress_callback=progress_adapter
            )

        # åˆå¹¶ç»“æœ
        logger.info("åˆå¹¶åˆ†ç±»å†³ç­–ç»“æœ...")
        classified_df = self.processor.merge_results(results)

        logger.info(f"åˆ†ç±»å†³ç­–å®Œæˆ: {len(classified_df):,} æ¡è®°å½•")
        return classified_df

    def _classify_chunk(self, chunk_df: pd.DataFrame) -> pd.DataFrame:
        """å¯¹æ•°æ®å—è¿›è¡Œåˆ†ç±»å†³ç­– - ä½¿ç”¨å‘é‡åŒ–æ“ä½œ"""
        df = chunk_df.copy()

        # 1. é¦–å…ˆè¿›è¡Œé›†ç¾¤è¿‡æ»¤
        df = self._apply_cluster_filtering(df)

        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ•°æ®ï¼Œç›´æ¥è¿”å›
        if len(df) == 0:
            return df

        # 2. è®¡ç®—å‡†ç¡®çš„èµ„æºä¿¡æ¯
        df = self._calculate_accurate_resources(df)

        # åˆå§‹åŒ–åˆ†ç±»å’Œç½®ä¿¡åº¦
        df['classification'] = 'unknown_category'
        df['confidence_score'] = 0.0

        thresholds = self.config['generation_filter']['classification']['confidence_thresholds']
        weights = self.config['generation_filter']['node_classification']['node_weights']
        node_weight = weights.get('possible_pattern', 3.0)
        script_weight = 1.0

        # 1. ç®¡ç†èŠ‚ç‚¹åˆ†ç±» (å‘é‡åŒ–)
        management_mask = df['is_management_node'] == True
        df.loc[management_mask, 'classification'] = 'management_nodes'
        df.loc[management_mask, 'confidence_score'] = 1.0

        # 2. æ˜ç¡®çš„äºŒæœŸèŠ‚ç‚¹åˆ†ç±» (å‘é‡åŒ–)
        decisive_mask = (df['node_classification_decisive'] == True) & (~management_mask)
        df.loc[decisive_mask, 'classification'] = 'second_generation_high'
        df.loc[decisive_mask, 'confidence_score'] = 0.95

        # 3. éœ€è¦ç»¼åˆåˆ¤æ–­çš„è®°å½• (å‘é‡åŒ–)
        remaining_mask = (~management_mask) & (~decisive_mask)
        remaining_df = df[remaining_mask].copy()

        if len(remaining_df) > 0:
            # è®¡ç®—åŠ æƒæ€»åˆ† (å‘é‡åŒ–)
            first_gen_total = (remaining_df['possible_first_gen_score'] * node_weight +
                             remaining_df['first_gen_script_score'] * script_weight)
            second_gen_total = (remaining_df['possible_second_gen_score'] * node_weight +
                              remaining_df['second_gen_script_score'] * script_weight)

            # è®¡ç®—ç½®ä¿¡åº¦ (å‘é‡åŒ–)
            total_score = first_gen_total + second_gen_total

            # äºŒæœŸæ•°æ®åˆ¤æ–­
            second_gen_mask = second_gen_total > first_gen_total
            second_gen_indices = remaining_df[second_gen_mask].index

            if len(second_gen_indices) > 0:
                confidence = second_gen_total[second_gen_mask] / (total_score[second_gen_mask] + 1e-6)
                confidence = confidence.fillna(0.5)

                # é«˜ç½®ä¿¡åº¦äºŒæœŸ
                high_conf_mask = confidence >= thresholds['high_confidence']
                high_conf_indices = second_gen_indices[high_conf_mask]
                df.loc[high_conf_indices, 'classification'] = 'second_generation_high'
                df.loc[high_conf_indices, 'confidence_score'] = confidence[high_conf_mask]

                # ä¸­ç­‰ç½®ä¿¡åº¦äºŒæœŸ
                medium_conf_mask = (confidence >= thresholds['medium_confidence']) & (confidence < thresholds['high_confidence'])
                medium_conf_indices = second_gen_indices[medium_conf_mask]
                df.loc[medium_conf_indices, 'classification'] = 'second_generation_medium'
                df.loc[medium_conf_indices, 'confidence_score'] = confidence[medium_conf_mask]

                # ä½ç½®ä¿¡åº¦äºŒæœŸ
                low_conf_mask = confidence < thresholds['medium_confidence']
                low_conf_indices = second_gen_indices[low_conf_mask]
                df.loc[low_conf_indices, 'classification'] = 'second_generation_low'
                df.loc[low_conf_indices, 'confidence_score'] = confidence[low_conf_mask]

            # ä¸€æœŸæ•°æ®åˆ¤æ–­
            first_gen_mask = (first_gen_total > second_gen_total) & (~second_gen_mask)
            first_gen_indices = remaining_df[first_gen_mask].index

            if len(first_gen_indices) > 0:
                confidence = first_gen_total[first_gen_mask] / (total_score[first_gen_mask] + 1e-6)
                confidence = confidence.fillna(0.5)
                df.loc[first_gen_indices, 'classification'] = 'first_generation'
                df.loc[first_gen_indices, 'confidence_score'] = confidence

            # æ··åˆç‰¹å¾åˆ¤æ–­
            mixed_mask = (first_gen_total > 0) & (second_gen_total > 0) & (first_gen_total == second_gen_total)
            mixed_indices = remaining_df[mixed_mask].index
            df.loc[mixed_indices, 'classification'] = 'mixed_features'
            df.loc[mixed_indices, 'confidence_score'] = 0.5

        return df
    
    def _separate_categories(self, df: pd.DataFrame, output_dir: str) -> Dict[str, str]:
        """åˆ†ç¦»ä¸åŒç±»åˆ«çš„æ•°æ®"""
        output_files = {}
        
        for category in self.config['generation_filter']['classification']['output_categories']:
            category_df = df[df['classification'] == category].copy()
            
            if len(category_df) > 0:
                # ç§»é™¤åˆ†æç”¨çš„ä¸´æ—¶åˆ—
                columns_to_drop = [
                    'first_gen_script_score', 'second_gen_script_score',
                    'definitive_second_gen_score', 'possible_first_gen_score', 'possible_second_gen_score',
                    'is_management_node', 'node_classification_decisive', 'classification', 'confidence_score'
                ]
                category_df = category_df.drop(columns=columns_to_drop, errors='ignore')
                
                # ä¿å­˜æ–‡ä»¶
                output_file = os.path.join(output_dir, f"{category}.csv")
                category_df.to_csv(output_file, index=False)
                output_files[category] = output_file
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats[category] = len(category_df)
                
                logger.info(f"ä¿å­˜ {category}: {len(category_df):,} æ¡è®°å½• -> {output_file}")
        
        return output_files
    
    def _generate_classification_report(self, output_dir: str):
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        report_file = os.path.join(output_dir, "classification_report.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=== ä¸€æœŸäºŒæœŸæ•°æ®åˆ†ç±»æŠ¥å‘Š ===\n\n")
            f.write(f"æ€»ä½œä¸šæ•°: {self.stats['total_jobs']:,}\n\n")
            
            f.write("åˆ†ç±»ç»“æœ:\n")
            for category, count in self.stats.items():
                if category != 'total_jobs' and count > 0:
                    percentage = count / self.stats['total_jobs'] * 100
                    f.write(f"  {category}: {count:,} ({percentage:.2f}%)\n")
            
            f.write(f"\näºŒæœŸæ•°æ®æ€»è®¡: {self.stats['second_generation_high'] + self.stats['second_generation_medium'] + self.stats['second_generation_low']:,}\n")
            f.write(f"æ•°æ®ä¿ç•™ç‡: {(self.stats['second_generation_high'] + self.stats['second_generation_medium'] + self.stats['second_generation_low']) / self.stats['total_jobs'] * 100:.2f}%\n")
        
        logger.info(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
